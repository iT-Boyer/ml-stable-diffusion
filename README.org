* Core ML Stable Diffusion
:PROPERTIES:
:CUSTOM_ID: core-ml-stable-diffusion
:END:


è¿™ä¸ªä»“åº“åŒ…å«:
- =python~coreml~stable~diffusion=,ä¸€ä¸ªç”¨äºå°† PyTorch æ¨¡å‹è½¬æ¢ä¸º =Core ML= æ ¼å¼å¹¶åœ¨ Python ä¸­ä½¿ç”¨ Hugging Face [[&lt;<https://github.com/huggingface/>sers>][diffusers]] è¿›è¡Œå›¾åƒç”Ÿæˆçš„ Python è½¯ä»¶åŒ…
- =StableDiffusion=,ä¸€ä¸ª Swift è½¯ä»¶åŒ…,å¼€å‘äººå‘˜å¯ä»¥å°†å…¶æ·»åŠ åˆ° Xcode é¡¹ç›®ä¸­ä½œä¸ºä¾èµ–é¡¹,ä»¥åœ¨å…¶åº”ç”¨ç¨‹åºä¸­éƒ¨ç½²å›¾åƒç”ŸæˆåŠŸèƒ½ã€‚Swift è½¯ä»¶åŒ…ä¾èµ–äº =python_coreml_stable_diffusion= ç”Ÿæˆçš„ Core ML æ¨¡å‹æ–‡ä»¶

å¦‚æœåœ¨å®‰è£…æˆ–è¿è¡Œæ—¶é‡åˆ°é—®é¢˜,è¯·å‚é˜…[[#faq][FAQ]]éƒ¨åˆ†ã€‚è¯·åœ¨å¼€å§‹ä¹‹å‰å‚é˜…[[#system-requirements][ç³»ç»Ÿè¦æ±‚]]éƒ¨åˆ†ã€‚

***  System Requirements
:PROPERTIES:
:CUSTOM_ID: system-requirements
:END:
The following is recommended to use all the functionality in this
repository:

| Python | macOS | Xcode | iPadOS, iOS |
|--------+-------+-------+-------------|
| 3.8    | 13.1  | 14.3  | 16.2        |

**  ä½¿ç”¨ Hugging Face Hub ä¸Šçš„ç°æˆ Core ML æ¨¡å‹
:PROPERTIES:
:CUSTOM_ID: using-ready-made-core-ml-models-from-hugging-face-hub
:END:
 Hugging Face æ‰§è¡Œäº†[[#converting-models-to-coreml][è½¬æ¢æµç¨‹]]å¯¹ä»¥ä¸‹æ¨¡å‹è¿›è¡Œäº†è½¬æ¢,å¹¶åœ¨ Hub ä¸Šå…¬å¼€æä¾›äº† Core ML æƒé‡ã€‚å¦‚æœæ‚¨æƒ³è½¬æ¢ Hub ä¸Šå°šæœªæä¾›çš„ Stable Diffusion ç‰ˆæœ¬,è¯·å‚é˜…[[#converting-models-to-core-ml][å°†æ¨¡å‹è½¬æ¢ä¸ºCore ML]]ã€‚

- [[&lt;<https://hug>ace.co/apple/coreml-stable-diffusion-v1-4>][=CompVis/stable-diffusion-v1-4=]]
- [[<https://huggingface.co/apple/coreml-stable-diffusion-v1-5>][=runwayml/stable-diffusion-v1-5=]]
- [[&lt;<https://huggingface.co>e/coreml-stable-diffusion-2-base>][=stabilityai/stable-diffusion-2-base=]]

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ä»»ä½•è¿™äº›æ¨¡å‹,æ‚¨å¯ä»¥ä¸‹è½½æƒé‡,ç„¶åç»§ç»­[[#image-generation-with-python][ä½¿ç”¨Pythonç”Ÿæˆå›¾åƒ]]æˆ–[[#image-generation-with-swift][Swift]]ã€‚

æ¯ä¸ªæ¨¡å‹ä»“åº“ä¸­éƒ½æœ‰å‡ ä¸ªå˜ç§ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ =git= å’Œ =git lfs= å°†æ•´ä¸ªä»“åº“å…‹éš†ä¸‹æ¥ä»¥ä¸‹è½½æ‰€æœ‰å˜ç§,æˆ–è€…é€‰æ‹©æ€§åœ°ä¸‹è½½æ‚¨éœ€è¦çš„å˜ç§ã€‚

è¦ä½¿ç”¨ =git= å…‹éš†ä»“åº“,è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œ:

1. *æ­¥éª¤1:* ä¸ºæ‚¨çš„ç³»ç»Ÿå®‰è£… =git lfs= æ‰©å±•ã€‚

=git lfs= å°†å¤§æ–‡ä»¶å­˜å‚¨åœ¨ä¸» git ä»“åº“ä¹‹å¤–,å¹¶åœ¨æ‚¨å…‹éš†æˆ–ç­¾å‡ºåä»ç›¸åº”çš„æœåŠ¡å™¨ä¸‹è½½å®ƒä»¬ã€‚å®ƒåœ¨å¤§å¤šæ•°åŒ…ç®¡ç†å™¨ä¸­éƒ½å¯ç”¨,æœ‰å…³è¯¦ç»†ä¿¡æ¯,è¯·æŸ¥çœ‹[[<https://git-lfs.com>][å®‰è£…é¡µé¢]]ã€‚

2. *Step 2:* Enable =git lfs= by running this command once:
    #+begin_src sh
git lfs install
    #+end_src

3. *Step 3:*
    å¯¹äºStable Diffusion 1.4ç‰ˆæœ¬,æ‚¨å°†åœ¨ç»ˆç«¯ä¸­å‘å‡ºä»¥ä¸‹å‘½ä»¤:
    #+begin_src sh
git clone https://huggingface.co/apple/coreml-stable-diffusion-v1-4
#+end_src

è¿™å°†ä¸‹è½½åŒ…å«æ‰€æœ‰æ¨¡å‹å˜ä½“çš„ä»“åº“å‰¯æœ¬ã€‚

å¦‚æœæ‚¨æ›´å–œæ¬¢ä¸‹è½½ç‰¹å®šçš„å˜ä½“è€Œä¸æ˜¯å…‹éš†ä»“åº“,å¯ä»¥ä½¿ç”¨ =huggingface_hub= Python åº“ã€‚ä¾‹å¦‚,è¦ä½¿ç”¨ Python è¿›è¡Œç”Ÿæˆå¹¶ä½¿ç”¨ =ORIGINAL= æ³¨æ„å®ç°(æœ‰å…³è¯¦ç»†ä¿¡æ¯,è¯·é˜…è¯»[[#converting-models-to-core-ml][æ­¤éƒ¨åˆ†]]),æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å¸®åŠ©ä»£ç :

#+begin_src python
from huggingface_hub import snapshot_download
from huggingface_hub.file_download import repo_folder_name
from pathlib import Path
import shutil

repo_id = "apple/coreml-stable-diffusion-v1-4"
variant = "original/packages"

def download_model(repo_id, variant, output_dir):
    destination = Path(output_dir) / (repo_id.split("/")[-1] + "_" + variant.replace("/", "_"))
    if destination.exists():
        raise Exception(f"Model already exists at {destination}")
    
    # Download and copy without symlinks
    downloaded = snapshot_download(repo_id, allow_patterns=f"{variant}/*", cache_dir=output_dir)
    downloaded_bundle = Path(downloaded) / variant
    shutil.copytree(downloaded_bundle, destination)

    # Remove all downloaded files
    cache_folder = Path(output_dir) / repo_folder_name(repo_id=repo_id, repo_type="model")
    shutil.rmtree(cache_folder)
    return destination

model_path = download_model(repo_id, variant, output_dir="./models")
print(f"Model downloaded at {model_path}")
#+end_src

 =model_path= å°†æ˜¯æ£€æŸ¥ç‚¹ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿçš„è·¯å¾„ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯,è¯·å‚é˜…[[<https://huggingface.co/blog/diffusers-coreml>][æ­¤å¸–å­]]ã€‚



**  å°†æ¨¡å‹è½¬æ¢ä¸º Core ML
:PROPERTIES:
:CUSTOM_ID: converting-models-to-core-ml
:END:

1. *Step 1:* åˆ›å»º Python ç¯å¢ƒå¹¶å®‰è£…ä¾èµ–é¡¹:
    #+begin_src sh
conda create -n coreml_stable_diffusion python=3.8 -y
conda activate coreml_stable_diffusion
cd /path/to/cloned/ml-stable-diffusion/repository
pip install -e .
    #+end_src

2. *Step 2:* ç™»å½•æˆ–æ³¨å†Œä½ çš„ =hugging Face= å¸å·,ç”Ÿæˆä¸€ä¸ªç”¨æˆ·è®¿é—®ä»¤ç‰Œå’Œä½¿ç”¨è¿™ä¸ªä»¤ç‰Œè®¾ç½®æ‹¥æŠ±è„¸ API åœ¨ç»ˆç«¯çª—å£ä¸­è¿è¡Œ ~~huggingface-cli login~ ç™»å½•ã€‚
3. *Step 3:* å¯¼èˆªåˆ°ç¨³å®šçš„ç‰ˆæœ¬ =hugging Face= æ‰©æ•£,æ‚¨æƒ³ä½¿ç”¨ä¸­å¿ƒå¹¶æ¥å—ä½¿ç”¨æ¡æ¬¾ã€‚é»˜è®¤çš„æ¨¡å‹ç‰ˆæœ¬æ˜¯ =CompVis/stable-diffusion-v1-4= ã€‚æ¨¡å‹ç‰ˆæœ¬å¯èƒ½æ”¹å˜äº†ç”¨æˆ·ï¼Œä¸‹ä¸€æ­¥ä¸­æè¿°ã€‚
4. *Step 4:* ä»ç»ˆç«¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ä»¥ç”Ÿæˆ Core ML model æ–‡ä»¶ï¼š(=.mlpackage=)
    #+begin_src shell
python -m python_coreml_stable_diffusion.torch2coreml --convert-unet --convert-text-encoder --convert-vae-decoder --convert-safety-checker -o <output-mlpackages-directory>
    #+end_src

*WARNING:* è¿™ä¸ªå‘½ä»¤å°†ä» Hugging Face ä¸‹è½½å‡ ä¸ªGBçš„ PyTorch æ£€æŸ¥ç‚¹ã€‚è¯·ç¡®ä¿æ‚¨è¿æ¥ Wi-Fi ä¸”æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ã€‚

è¿™é€šå¸¸åœ¨ M1 MacBook Pro ä¸Šéœ€è¦15-20åˆ†é’Ÿã€‚æˆåŠŸæ‰§è¡Œå, Stable Diffusion ç”±4ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹ç»„æˆ,å°†ä» PyTorch è½¬æ¢ä¸º Core ML(=.mlpackage=)å¹¶ä¿å­˜åˆ°æŒ‡å®šçš„ =<output-mlpackages-directory>= ã€‚ä¸€äº›å…¶ä»–å€¼å¾—æ³¨æ„çš„å‚æ•°:

- =--precision= - å®šç‚¹ç²¾åº¦ã€‚é»˜è®¤ä¸º"float32",ä¹Ÿå¯ä»¥æ˜¯"float16"ä»¥è¿›ä¸€æ­¥ç¼©å°æ¨¡å‹å¤§å°ã€‚
- =--model-version= - Hubæ¨¡å‹IDã€‚é»˜è®¤ä¸º =CompVis/stable-diffusion-v1-4= ã€‚è¦è½¬æ¢å…¶ä»–æ¨¡å‹,è¯·æŒ‡å®šå…¶ Hub IDã€‚
- =--chunk-unet= - å°† UNet æƒé‡æ‹†åˆ†ä¸ºå¤šä¸ªæ–‡ä»¶ä»¥å…¼å®¹ iOS å’Œ iPadOS è®¾å¤‡ã€‚
- =--no-quantized= - ç¦ç”¨å®šç‚¹ã€‚é»˜è®¤æƒ…å†µä¸‹,è½¬æ¢è„šæœ¬ä¼šæ‰§è¡Œç®€å•çš„æƒé‡å®šç‚¹ä»¥å‡å°æ¨¡å‹å¤§å°ã€‚ä½¿ç”¨æ­¤æ ‡å¿—å¯ä»¥ç¦ç”¨å®ƒã€‚
- =--cache-dir= - ç¼“å­˜ç›®å½•ä»¥é‡å¤ä½¿ç”¨ä¸‹è½½çš„æƒé‡ã€‚è¿™å¯ä»¥èŠ‚çœæ—¶é—´å’Œå¸¦å®½ã€‚

ä¸€ä¸ªç¤ºä¾‹å‘½ä»¤å¯èƒ½å¦‚ä¸‹:
#+BEGIN_SRC elisp -n 1 :hl_lines 0-0,0-0
python convert_to_coreml.py
    --precision float16
    --model-version CompVis/stable-diffusion-v1-4
    --chunk-unet
    --no-quantized
    --cache-dir ./cache
    <output-mlpackages-directory>
#+END_SRC
- =--model-version=: æ¨¡å‹ç‰ˆæœ¬é»˜è®¤ä¸º[[<https://huggingface.co/CompVis/stable-diffusion-v1-4>][CompVis/stable-diffusion-v1-4]]ã€‚å¼€å‘äººå‘˜å¯ä»¥æŒ‡å®š Hugging Face Hub ä¸Šæä¾›çš„å…¶ä»–ç‰ˆæœ¬,ä¾‹å¦‚[[<https://huggingface.co/stabilityai/stable-diffusion-2-base>][stabilityai/stable-diffusion-2-base]]å’Œ[[<https://huggingface.co/runwayml/stable-diffusion-v1-5>][runwayml/stable-diffusion-v1-5]]ã€‚

- =--bundle-resources-for-swift-cli=: ç¼–è¯‘æ‰€æœ‰4ä¸ªæ¨¡å‹å¹¶å°†å…¶ä¸æ–‡æœ¬æ ‡è®°åŒ–æ‰€éœ€çš„èµ„æºä¸€èµ·æ‰“åŒ…åˆ° =<output-mlpackages-directory>/Resources= ä¸­,è¯¥èµ„æºåº”ä½œä¸º Swift åŒ…çš„è¾“å…¥æä¾›ã€‚å¯¹äºåŸºäº diffusers çš„ Python ç®¡é“,æ­¤æ ‡å¿—ä¸æ˜¯å¿…éœ€çš„ã€‚

- =--chunk-unet=: å°† Unet æ¨¡å‹æ‹†åˆ†ä¸ºä¸¤ä¸ªå¤§è‡´ç›¸ç­‰çš„å—(æ¯ä¸ªå—çš„æƒé‡å°‘äº1GB),ä»¥ä¾¿ç§»åŠ¨å‹å¥½éƒ¨ç½²ã€‚è¿™å¯¹ iOS å’Œ iPadOS ä¸Šçš„ç¥ç»å¼•æ“éƒ¨ç½²*å¿…éœ€*ã€‚è¿™å¯¹ macOS ä¸æ˜¯å¿…éœ€çš„ã€‚Swift CLI èƒ½å¤Ÿæ¶ˆè€— Unet æ¨¡å‹çš„å—åŒ–ç‰ˆæœ¬å’Œå¸¸è§„ç‰ˆæœ¬,ä½†ä¼˜å…ˆè€ƒè™‘å‰è€…ã€‚è¯·æ³¨æ„,å— UNet ä¸ Python ç®¡é“ä¸å…¼å®¹,å› ä¸º Python ç®¡é“ä»…ç”¨äº macOS ã€‚åˆ†å—ä»…ç”¨äº Swift è¿›è¡Œè®¾å¤‡éƒ¨ç½²ã€‚

- =--attention-implementation=: é»˜è®¤ä¸º =SPLIT_EINSUM= ,è¿™æ˜¯[[<https://machinelearning.apple.com/research/neural-engine-transformers>][åœ¨è‹¹æœç¥ç»å¼•æ“ä¸Šéƒ¨ç½²Transformers]]ä¸­æè¿°çš„å®ç°ã€‚ =--attention-implementation ORIGINAL= å°†åˆ‡æ¢åˆ°å¦ä¸€ç§æ›¿ä»£æ–¹æ³•,åº”ç”¨äºCPUæˆ–GPUéƒ¨ç½²ã€‚æœ‰å…³è¿›ä¸€æ­¥æŒ‡å¯¼,è¯·å‚é˜…[[#performance-benchmark][æ€§èƒ½åŸºå‡†]]éƒ¨åˆ†ã€‚

- =--check-output-correctness=: å°†åŸå§‹ PyTorch æ¨¡å‹çš„è¾“å‡ºä¸æœ€ç»ˆ Core ML æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œæ¯”è¾ƒã€‚æ­¤æ ‡å¿—ä¼šæ˜¾ç€å¢åŠ RAMæ¶ˆè€—,å› æ­¤ä»…å»ºè®®ç”¨äºè°ƒè¯•ç›®çš„ã€‚

- =--convert-controlnet=: è½¬æ¢åœ¨æ­¤é€‰é¡¹åæŒ‡å®šçš„ ControlNet æ¨¡å‹ã€‚å¦‚æœæŒ‡å®š =--convert-controlnet lllyasviel/sd-controlnet-mlsd lllyasviel/sd-controlnet-depth= ,è¿™ä¹Ÿå¯ä»¥è½¬æ¢å¤šä¸ªæ¨¡å‹ã€‚

- =--unet-support-controlnet=: å¯ç”¨è½¬æ¢åçš„ UNet æ¨¡å‹æ¥æ”¶æ¥è‡ª ControlNet çš„é¢å¤–è¾“å…¥ã€‚è¿™éœ€è¦ä½¿ç”¨ ControlNet ç”Ÿæˆå›¾åƒå¹¶ä»¥ä¸åŒçš„åç§° =*_control-unet.mlpackage= ä¿å­˜,ä¸æ­£å¸¸çš„ UNet ä¸åŒã€‚å¦ä¸€æ–¹é¢,å¦‚æœæ²¡æœ‰ ControlNet,æ­¤ UNet æ¨¡å‹æ— æ³•å·¥ä½œã€‚è¯·ä»…ä½¿ç”¨å¸¸è§„ UNet è¿›è¡Œtxt2imgã€‚

**  Image Generation with Python
:PROPERTIES:
:CUSTOM_ID: image-generation-with-python
:END:

ä½¿ç”¨åŸºäº [[https://github.com/huggingface/diffusers][diffusers]] çš„ç¤ºä¾‹Pythonç®¡é“è¿›è¡Œ text-to-image æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ­¥éª¤:
#+begin_src shell
python -m python_coreml_stable_diffusion.pipeline --prompt "a photo of an astronaut riding a horse on mars" -i <output-mlpackages-directory> -o </path/to/output/image> --compute-unit ALL --seed 93
#+end_src
è¿™é‡Œæ˜¯ =python -m python_coreml_stable_diffusion.pipeline -h= çš„ä¸€äº›å€¼å¾—æ³¨æ„çš„å‚æ•°:

- =-i=: åº”æŒ‡å‘ä¸Šé¢ "å°†æ¨¡å‹è½¬æ¢ä¸º Core ML" éƒ¨åˆ†ç¬¬4æ­¥çš„ =-o= ç›®å½•ã€‚
- =--model-version=: å¦‚æœåœ¨å°†æ¨¡å‹è½¬æ¢ä¸º Core ML æ—¶è¦†ç›–äº†é»˜è®¤çš„æ¨¡å‹ç‰ˆæœ¬,åˆ™éœ€è¦åœ¨è¿™é‡ŒæŒ‡å®šç›¸åŒçš„æ¨¡å‹ç‰ˆæœ¬ã€‚
- =--compute-unit=: è¯·æ³¨æ„,å¯¹äºè¿™ä¸ªç‰¹å®šçš„å®ç°,æ€§èƒ½æœ€å¥½çš„è®¡ç®—å•å…ƒåœ¨ä¸åŒçš„ç¡¬ä»¶ä¸Šå¯èƒ½ä¸åŒã€‚ =CPU_AND_GPU= æˆ– =CPU_AND_NE= å¯èƒ½æ¯” =ALL= æ›´å¿«ã€‚æœ‰å…³è¿›ä¸€æ­¥æŒ‡å¯¼,è¯·å‚é˜… â€œæ€§èƒ½åŸºå‡†â€ éƒ¨åˆ†ã€‚
- =--scheduler=: å¦‚æœæ‚¨æƒ³å°è¯•ä¸åŒçš„è°ƒåº¦ç¨‹åº,å¯ä»¥åœ¨è¿™é‡ŒæŒ‡å®šã€‚æœ‰å…³å¯ç”¨é€‰é¡¹,è¯·å‚é˜…å¸®åŠ©èœå•ã€‚æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡ =--num-inference-steps= æŒ‡å®šè‡ªå®šä¹‰çš„æ¨ç†æ­¥æ•°,é»˜è®¤ä¸º50ã€‚
- =--controlnet=: ä½¿ç”¨æ­¤é€‰é¡¹æŒ‡å®šçš„ ControlNet æ¨¡å‹ç”¨äºå›¾åƒç”Ÿæˆã€‚ä½¿ç”¨æ ¼å¼ =--controlnet lllyasviel/sd-controlnet-mlsd lllyasviel/sd-controlnet-depth= å¹¶ç¡®ä¿ä¸ =--controlnet-inputs= ä¸€èµ·ä½¿ç”¨ã€‚
- =--controlnet-inputs=: ä¸æ¯ä¸ª ControlNet æ¨¡å‹å¯¹åº”çš„å›¾åƒè¾“å…¥ã€‚è¯·ä»¥ =--controlnet= ä¸­çš„æ¨¡å‹ç›¸åŒçš„é¡ºåºæä¾›å›¾åƒè·¯å¾„,ä¾‹å¦‚:
  =--controlnet-inputs image_mlsd image_depth= ã€‚

é™¤äº†ä¸Šé¢åˆ—å‡ºçš„ä¸€äº›ä¸»è¦å‚æ•°å¤–,è¿˜æœ‰è®¸å¤šå…¶ä»–å‚æ•°å¯ç”¨äºè°ƒæ•´å’Œä¼˜åŒ–å›¾åƒç”Ÿæˆç®¡é“ã€‚æˆ‘å¼ºçƒˆå»ºè®®æ‚¨æµè§ˆè¯¥è½¯ä»¶åŒ…çš„å¸®åŠ©èœå•,äº†è§£æ‰€æœ‰å¯ç”¨å‚æ•°åŠå…¶åŠŸèƒ½ã€‚è¯·è®©æˆ‘çŸ¥é“å¦‚æœæ‚¨æœ‰ä»»ä½•å…¶ä»–é—®é¢˜!
**  Image Generation with Swift

***  System Requirements
:PROPERTIES:
:CUSTOM_ID: system-requirements-1
:END:
*Building* (minimum):

- Xcode 14.3
- Command Line Tools for Xcode 14.3

Check [[https://developer.apple.com/download/all/?q=xcode][developer.apple.com]] for the latest versions.

*Running* (minimum):

| Mac        | iPad*       | iPhone*       |
|------------+-------------+---------------|
| macOS 13.1 | iPadOS 16.2 | iOS 16.2      |
| M1         | M1          | iPhone 12 Pro |

You will also need the resources generated by the =--bundle-resources-for-swift-cli= option described in
[[#converting-models-to-coreml][Converting Models to Core ML]]

Please see [[#faq][FAQ]] [[#q-mobile-app][Q6]] regarding deploying on iPad and iPhone.

*** Example CLI Usage
:PROPERTIES:
:CUSTOM_ID: example-cli-usage
:END:
#+begin_src shell
swift run StableDiffusionSample "a photo of an astronaut riding a horse on mars" --resource-path <output-mlpackages-directory>/Resources/ --seed 93 --output-path </path/to/output/image>
#+end_src

è¾“å‡ºå°†æ ¹æ®æç¤ºå’Œéšæœºç§å­å‘½å:

ä¾‹å¦‚: =</path/to/output/image>/a_photo_of_an_astronaut_riding_a_horse_on_mars.93.final.png=

è¯·ä½¿ç”¨ =--help= æ ‡å¿—äº†è§£æ‰¹é‡ç”Ÿæˆå’Œæ›´å¤šä¿¡æ¯ã€‚

*** Example Library Usage
:PROPERTIES:
:CUSTOM_ID: example-library-usage
:END:
#+begin_src swift
import StableDiffusion
...
let pipeline = try StableDiffusionPipeline(resourcesAt: resourceURL)
pipeline.loadResources()
let image = try pipeline.generateImages(prompt: prompt, seed: seed).first
#+end_src

åœ¨ iOS ä¸Š,æ„é€  =StableDiffusionPipeline= æ—¶,åº”å°† =reduceMemory= é€‰é¡¹è®¾ç½®ä¸º =true= ã€‚

*** Swift Package Details
:PROPERTIES:
:CUSTOM_ID: swift-package-details
:END:
This Swift package contains two products:

- =StableDiffusion= library
- =StableDiffusionSample= command-line tool

Both of these products require the Core ML models and tokenization
resources to be supplied. When specifying resources via a directory path
that directory must contain the following:

- =TextEncoder.mlmodelc= (text embedding model)
- =Unet.mlmodelc= or =UnetChunk1.mlmodelc= & =UnetChunk2.mlmodelc=
  (denoising autoencoder model)
- =VAEDecoder.mlmodelc= (image decoder model)
- =vocab.json= (tokenizer vocabulary file)
- =merges.text= (merges for byte pair encoding file)

Optionally, for image2image, in-painting, or similar:

- =VAEEncoder.mlmodelc= (image encoder model)

Optionally, it may also include the safety checker model that some
versions of Stable Diffusion include:

- =SafetyChecker.mlmodelc=

Optionally, for ControlNet:

- =ControlledUNet.mlmodelc= or =ControlledUnetChunk1.mlmodelc= &
  =ControlledUnetChunk2.mlmodelc= (enabled to receive ControlNet values)
- =controlnet/= (directory containing ControlNet models)
  - =LllyasvielSdControlnetMlsd.mlmodelc= (for example, from
    lllyasviel/sd-controlnet-mlsd)
  - =LllyasvielSdControlnetDepth.mlmodelc= (for example, from
    lllyasviel/sd-controlnet-depth)
  - Other models you converted

Note that the chunked version of Unet is checked for first. Only if it
is not present will the full =Unet.mlmodelc= be loaded. Chunking is
required for iOS and iPadOS and not necessary for macOS.

æ­¤ Swift åŒ…åŒ…å«ä¸¤ä¸ªäº§å“:

- =StableDiffusion= åº“
- =StableDiffusionSample= å‘½ä»¤è¡Œå·¥å…·

è¿™ä¸¤ä¸ªäº§å“éƒ½éœ€è¦æä¾› Core ML æ¨¡å‹å’Œä»¤ç‰ŒåŒ–èµ„æºã€‚å½“é€šè¿‡ç›®å½•è·¯å¾„æŒ‡å®šèµ„æºæ—¶,è¯¥ç›®å½•å¿…é¡»åŒ…å«ä»¥ä¸‹å†…å®¹:

- =TextEncoder.mlmodelc= (æ–‡æœ¬åµŒå…¥æ¨¡å‹)
- =Unet.mlmodelc= æˆ– =UnetChunk1.mlmodelc= & =UnetChunk2.mlmodelc= (é™å™ªè‡ªåŠ¨ç¼–ç å™¨æ¨¡å‹)
- =VAEDecoder.mlmodelc= (å›¾åƒè§£ç å™¨æ¨¡å‹)
- =vocab.json= (ä»¤ç‰Œå™¨è¯æ±‡æ–‡ä»¶)
- =merges.text= (ç”¨äºå­—å¯¹ç¼–ç çš„åˆå¹¶æ–‡ä»¶)

å¯é€‰åœ°,å¯¹äºimage2imageã€ä¿®è¡¥æˆ–ç±»ä¼¼çš„åŠŸèƒ½:

- =VAEEncoder.mlmodelc= (å›¾åƒç¼–ç å™¨æ¨¡å‹)

å¯é€‰åœ°,å®ƒè¿˜å¯ä»¥åŒ…æ‹¬æŸäº›ç‰ˆæœ¬çš„ Stable Diffusion æ‰€åŒ…å«çš„å®‰å…¨æ€§æ£€æŸ¥å™¨æ¨¡å‹:

- =SafetyChecker.mlmodelc=

å¯é€‰åœ°,å¯¹äº ControlNet:

- =ControlledUNet.mlmodelc= æˆ– =ControlledUnetChunk1.mlmodelc= & =ControlledUnetChunk2.mlmodelc= (å¯ç”¨ä»¥æ¥æ”¶ ControlNet å€¼)
- =controlnet/= (åŒ…å« ControlNet æ¨¡å‹çš„ç›®å½•)
  - =LllyasvielSdControlnetMlsd.mlmodelc= (ä¾‹å¦‚,æ¥è‡ª lllyasviel/sd-controlnet-mlsd)
  - =LllyasvielSdControlnetDepth.mlmodelc= (ä¾‹å¦‚,æ¥è‡ª lllyasviel/sd-controlnet-depth)
  - æ‚¨è½¬æ¢çš„å…¶ä»–æ¨¡å‹

è¯·æ³¨æ„,é¦–å…ˆæ£€æŸ¥ Unet çš„å—çŠ¶ç‰ˆæœ¬ã€‚åªæœ‰å½“å…¶ä¸å­˜åœ¨æ—¶,æ‰ä¼šåŠ è½½å®Œæ•´çš„ =Unet.mlmodelc= ã€‚åˆ†å—å¯¹ iOS å’Œ iPadOS æ˜¯å¿…éœ€çš„,å¯¹ macOS ä¸æ˜¯å¿…éœ€çš„ã€‚

**  Example Swift App
:PROPERTIES:
:CUSTOM_ID: example-swift-app
:END:

ğŸ¤— Hugging Face created an
[[https://github.com/huggingface/swift-coreml-diffusers][open-source
demo app]] on top of this library. It's written in native Swift and
Swift UI, and runs on macOS, iOS and iPadOS. You can use the code as a
starting point for your app, or to see how to integrate this library in
your own projects.

Hugging Face has made the app
[[https://apps.apple.com/app/diffusers/id1666309574?mt=12][available in
the Mac App Store]].

**  Performance Benchmark
:PROPERTIES:
:CUSTOM_ID: performance-benchmark
:END:

Standard
[[https://huggingface.co/CompVis/stable-diffusion-v1-4][CompVis/stable-diffusion-v1-4]]
Benchmark

| Device                             | =--compute-unit= | =--attention-implementation= | Latency (seconds) |
|------------------------------------+------------------+------------------------------+-------------------|
| Mac Studio (M1 Ultra, 64-core GPU) | =CPU_AND_GPU=    | =ORIGINAL=                   | 9                 |
| Mac Studio (M1 Ultra, 48-core GPU) | =CPU_AND_GPU=    | =ORIGINAL=                   | 13                |
| MacBook Pro (M1 Max, 32-core GPU)  | =CPU_AND_GPU=    | =ORIGINAL=                   | 18                |
| MacBook Pro (M1 Max, 24-core GPU)  | =CPU_AND_GPU=    | =ORIGINAL=                   | 20                |
| MacBook Pro (M1 Pro, 16-core GPU)  | =ALL=            | =SPLIT_EINSUM (default)=     | 26                |
| MacBook Pro (M2)                   | =CPU_AND_NE=     | =SPLIT_EINSUM (default)=     | 23                |
| MacBook Pro (M1)                   | =CPU_AND_NE=     | =SPLIT_EINSUM (default)=     | 35                |
| iPad Pro (5th gen, M1)             | =CPU_AND_NE=     | =SPLIT_EINSUM (default)=     | 38                |

Please see [[#important-notes-on-performance-benchmarks][Important Notes
on Performance Benchmarks]] section for details.


**  Important Notes on Performance Benchmarks
:PROPERTIES:
:CUSTOM_ID: important-notes-on-performance-benchmarks
:END:

- This benchmark was conducted by Apple using public beta versions of
  iOS 16.2, iPadOS 16.2 and macOS 13.1 in November 2022.
- The executed program is =python_coreml_stable_diffusion.pipeline= for
  macOS devices and a minimal Swift test app built on the
  =StableDiffusion= Swift package for iOS and iPadOS devices.
- The median value across 3 end-to-end executions is reported.
- Performance may materially differ across different versions of Stable
  Diffusion due to architecture changes in the model itself. Each
  reported number is specific to the model version mentioned in that
  context.
- The image generation procedure follows the standard configuration: 50
  inference steps, 512x512 output image resolution, 77 text token
  sequence length, classifier-free guidance (batch size of 2 for unet).
- The actual prompt length does not impact performance because the Core
  ML model is converted with a static shape that computes the forward
  pass for all of the 77 elements (=tokenizer.model_max_length=) in the
  text token sequence regardless of the actual length of the input text.
- Pipelining across the 4 models is not optimized and these performance
  numbers are subject to variance under increased system load from other
  applications. Given these factors, we do not report sub-second
  variance in latency.
- Weights and activations are in float16 precision for both the GPU and
  the Neural Engine.
- The Swift CLI program consumes a peak memory of approximately 2.6GB
  (without the safety checker), 2.1GB of which is model weights in
  float16 precision. We applied
  [[https://coremltools.readme.io/docs/compressing-ml-program-weights#use-affine-quantization][8-bit
  weight quantization]] to reduce peak memory consumption by
  approximately 1GB. However, we observed that it had an adverse effect
  on generated image quality and we rolled it back. We encourage
  developers to experiment with other advanced weight compression
  techniques such as
  [[https://coremltools.readme.io/docs/compressing-ml-program-weights#use-a-lookup-table][palettization]]
  and/or
  [[https://coremltools.readme.io/docs/compressing-ml-program-weights#use-sparse-representation][pruning]]
  which may yield better results.
- In the [[file:performance-benchmark][benchmark table]], we report the
  best performing =--compute-unit= and =--attention-implementation=
  values per device. The former does not modify the Core ML model and
  can be applied during runtime. The latter modifies the Core ML model.
  Note that the best performing compute unit is model version and
  hardware-specific.

**  Results with Different Compute Units
:PROPERTIES:
:CUSTOM_ID: results-with-different-compute-units
:END:

It is highly probable that there will be slight differences across
generated images using different compute units.

The following images were generated on an M1 MacBook Pro and macOS 13.1
with the prompt /"a photo of an astronaut riding a horse on mars"/ using
the
[[https://huggingface.co/runwayml/stable-diffusion-v1-5][runwayml/stable-diffusion-v1-5]]
model version. The random seed was set to 93:

| CPU_AND_NE                                                                                                                                                        | CPU_AND_GPU                                                                                                                                                        | ALL                                                                                                                                                        |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [[file:assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_93_computeUnit_CPU_AND_NE_modelVersion_runwayml_stable-diffusion-v1-5.png]] | [[file:assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_93_computeUnit_CPU_AND_GPU_modelVersion_runwayml_stable-diffusion-v1-5.png]] | [[file:assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_93_computeUnit_ALL_modelVersion_runwayml_stable-diffusion-v1-5.png]] |

Differences may be less or more pronounced for different inputs. Please
see the [[#faq][FAQ]] Q8 for a detailed explanation.

**  Results with ControlNet
:PROPERTIES:
:CUSTOM_ID: results-with-controlnet
:END:

[[https://huggingface.co/lllyasviel/ControlNet][ControlNet]] allows
users to condition image generation with Stable Diffusion on signals
such as edge maps, depth maps, segmentation maps, scribbles and pose.
Thanks to
[[https://github.com/apple/ml-stable-diffusion/pull/153][[cite/t:@ryu38]'s
contribution]], both the Python CLI and the Swift package support
ControlNet models. Please refer to CLI arguments in previous sections to
exercise this new feature.

Example results using the prompt "a high quality photo of a surfing dog"
conditioned on the scribble (leftmost):

**  FAQ
:PROPERTIES:
:CUSTOM_ID: faq
:END:

Q1: ERROR: Failed building wheel for tokenizers or error: can't find
Rust compiler


A1: Please review this
[[https://github.com/huggingface/transformers/issues/2831#issuecomment-592724471][potential solution]].


Q2: RuntimeError: {NSLocalizedDescription = "Error computing NN
outputs."


A2: There are many potential causes for this error. In this context, it
is highly likely to be encountered when your system is under increased
memory pressure from other applications. Reducing memory utilization of
other applications is likely to help alleviate the issue.


Q3: My Mac has 8GB RAM and I am converting models to Core ML using the
example command. The process is getting killed because of memory issues.
How do I fix this issue?


A3: In order to minimize the memory impact of the model conversion
process, please execute the following command instead:

#+begin_src sh
python -m python_coreml_stable_diffusion.torch2coreml --convert-vae-encoder -o <output-mlpackages-directory> && \
python -m python_coreml_stable_diffusion.torch2coreml --convert-vae-decoder -o <output-mlpackages-directory> && \
python -m python_coreml_stable_diffusion.torch2coreml --convert-unet -o <output-mlpackages-directory> && \
python -m python_coreml_stable_diffusion.torch2coreml --convert-text-encoder -o <output-mlpackages-directory> && \
python -m python_coreml_stable_diffusion.torch2coreml --convert-safety-checker -o <output-mlpackages-directory> &&
#+end_src

If you need =--chunk-unet=, you may do so in yet another independent
command which will reuse the previously exported Unet model and simply
chunk it in place:

#+begin_src sh
python -m python_coreml_stable_diffusion.torch2coreml --convert-unet --chunk-unet -o <output-mlpackages-directory>
#+end_src

Q4: My Mac has 8GB RAM, should image generation work on my machine?


A4: Yes! Especially the =--compute-unit CPU_AND_NE= option should work
under reasonable system load from other applications. Note that part of
the [[#example-results][Example Results]] were generated using an M2
MacBook Air with 8GB RAM.


Q5: Every time I generate an image using the Python pipeline, loading
all the Core ML models takes 2-3 minutes. Is this expected?

A5: Yes and using the Swift library reduces this to just a few seconds.
The reason is that =coremltools= loads Core ML models (=.mlpackage=) and
each model is compiled to be run on the requested compute unit during
load time. Because of the size and number of operations of the unet
model, it takes around 2-3 minutes to compile it for Neural Engine
execution. Other models should take at most a few seconds. Note that
=coremltools= does not cache the compiled model for later loads so each
load takes equally long. In order to benefit from compilation caching,
=StableDiffusion= Swift package by default relies on compiled Core ML
models (=.mlmodelc=) which will be compiled down for the requested
compute unit upon first load but then the cache will be reused on
subsequent loads until it is purged due to lack of use.

If you intend to use the Python pipeline in an application, we recommend
initializing the pipeline once so that the load time is only incurred
once. Afterwards, generating images using different prompts and random
seeds will not incur the load time for the current session of your
application.

Q6: I want to deploy StableDiffusion, the Swift package, in my mobile
app. What should I be aware of?

A6: The [[#image-gen-swift][Image Generation with Swift]] section
describes the minimum SDK and OS versions as well as the device models
supported by this package. We recommend carefully testing the package on
the device with the least amount of RAM available among your deployment
targets.

The image generation process in =StableDiffusion= can yield over 2 GB of
peak memory during runtime depending on the compute units selected. On
iPadOS, we recommend using =.cpuAndNeuralEngine= in your configuration
and the =reduceMemory= option when constructing a
=StableDiffusionPipeline= to minimize memory pressure.

If your app crashes during image generation, consider adding the
[[https://developer.apple.com/documentation/bundleresources/entitlements/com_apple_developer_kernel_increased-memory-limit][Increased
Memory Limit]] capability to inform the system that some of your app's
core features may perform better by exceeding the default app memory
limit on supported devices.

On iOS, depending on the iPhone model, Stable Diffusion model versions,
selected compute units, system load and design of your app, this may
still not be sufficient to keep your apps peak memory under the limit.
Please remember, because the device shares memory between apps and iOS
processes, one app using too much memory can compromise the user
experience across the whole device.

Q7: How do I generate images with different resolutions using the same
Core ML models?

A7: The current version of =python_coreml_stable_diffusion= does not
support single-model multi-resolution out of the box. However,
developers may fork this project and leverage the
[[https://coremltools.readme.io/docs/flexible-inputs][flexible shapes]]
support from coremltools to extend the =torch2coreml= script by using
=coremltools.EnumeratedShapes=. Note that, while the =text_encoder= is
agnosti c to the image resolution, the inputs and outputs of
=vae_decoder= and =unet= models are dependent on the desired image
resolution.

Q8: Are the Core ML and PyTorch generated images going to be identical?

A8: If desired, the generated images across PyTorch and Core ML can be
made approximately identical. However, it is not guaranteed by default.
There are several factors that might lead to different images across
PyTorch and Core ML:

 1. Random Number Generator Behavior

The main source of potentially different results across PyTorch and Core
ML is the Random Number Generator
([[https://en.wikipedia.org/wiki/Random_number_generation][RNG]])
behavior. PyTorch and Numpy have different sources of randomness.
=python_coreml_stable_diffusion= generally relies on Numpy for RNG
(e.g.Â latents initialization) and =StableDiffusion= Swift Library
reproduces this RNG behavior by default. However, PyTorch-based
pipelines such as Hugging Face =diffusers= relies on PyTorch's RNG
behavior. Thanks to [cite/t:@liuliu]'s
[[https://github.com/apple/ml-stable-diffusion/pull/124][contribution]],
one can match the PyTorch (CPU) RNG behavior in Swift by specifying
=--rng torch= which selects the =torchRNG= mode.

 2. PyTorch

/"Completely reproducible results are not guaranteed across PyTorch
releases, individual commits, or different platforms. Furthermore,
results may not be reproducible between CPU and GPU executions, even
when using identical seeds."/
([[https://pytorch.org/docs/stable/notes/randomness.html#reproducibility][source]]).

 3. Model Function Drift During Conversion

The difference in outputs across corresponding PyTorch and Core ML
models is a potential cause. The signal integrity is tested during the
conversion process (enabled via =--check-output-correctness= argument to
=python_coreml_stable_diffusion.torch2coreml=) and it is verified to be
above a minimum
[[https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio][PSNR]] value
as tested on random inputs. Note that this is simply a sanity check and
does not guarantee this minimum PSNR across all possible inputs.
Furthermore, the results are not guaranteed to be identical when
executing the same Core ML models across different compute units. This
is not expected to be a major source of difference as the sample visual
results indicate in [[#results-with-different-compute-units][this section]].

 4. Weights and Activations Data Type

When quantizing models from float32 to lower-precision data types such
as float16, the generated images are
[[https://lambdalabs.com/blog/inference-benchmark-stable-diffusion][known to vary slightly]] in semantics even when using the same PyTorch model.
Core ML models generated by coremltools have float16 weights and
activations by default
[[https://github.com/apple/coremltools/blob/main/coremltools/converters/_converters_entry.py#L256][unless explicitly overridden]]. This is not expected to be a major source of
difference.

Q9: The model files are very large, how do I avoid a large binary for my
App?

A9: The recommended option is to prompt the user to download these
assets upon first launch of the app. This keeps the app binary size
independent of the Core ML models being deployed. Disclosing the size of
the download to the user is extremely important as there could be data
charges or storage impact that the user might not be comfortable with.

Q10: =Could not initialize NNPACK! Reason: Unsupported hardware=


A10: This warning is safe to ignore in the context of this repository.

Q11: TracerWarning: Converting a tensor to a Python boolean might cause
the trace to be incorrect


A11: This warning is safe to ignore in the context of this repository.

Q12: UserWarning: resource_tracker: There appear to be 1 leaked
semaphore objects to clean up at shutdown

A12: If this warning is printed right after zsh: killed python -m
python_coreml_stable_diffusion.torch2coreml ... , then it is highly
likely that your Mac has run out of memory while converting models to
Core ML. Please see [[#low-mem-conversion][Q3]] from above for the
solution.

